<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Vox - Google Gemini Style</title>
  <link rel="stylesheet" href="style.css">
</head>
  <style>
    body {
  margin: 0;
  font-family: Arial, sans-serif;
  background-color: #121212;
  color: #fff;
  display: flex;
  justify-content: center;
  align-items: center;
  height: 100vh;
}

.container {
  text-align: center;
}

#glow-indicator {
  width: 200px;
  height: 200px;
  margin: 20px auto;
  border-radius: 50%;
  background: radial-gradient(circle, rgba(0, 123, 255, 0.7), rgba(0, 123, 255, 0.3));
  box-shadow: 0 0 30px 10px rgba(0, 123, 255, 0.5);
  animation: glow-pulse 2s infinite ease-in-out;
  opacity: 0;
  transition: opacity 0.3s ease;
}

@keyframes glow-pulse {
  0% {
    box-shadow: 0 0 30px 10px rgba(0, 123, 255, 0.5);
  }
  50% {
    box-shadow: 0 0 50px 20px rgba(0, 123, 255, 0.7);
  }
  100% {
    box-shadow: 0 0 30px 10px rgba(0, 123, 255, 0.5);
  }
}

.controls button {
  background-color: #007bff;
  color: white;
  border: none;
  padding: 10px 20px;
  margin: 10px;
  cursor: pointer;
  border-radius: 5px;
  font-size: 1rem;
}

.controls button:hover {
  background-color: #0056b3;
}

#output {
  margin-top: 20px;
  font-size: 1.2rem;
}

  </style>
<body>
  <div class="container">
    <h1>Vox - Smart Voice Assistant</h1>
    <div id="glow-indicator" class="glow"></div>
    <div id="output">Click "Start" to begin.</div>
    <div class="controls">
      <button id="start">Start</button>
      <button id="hold">Hold</button>
      <button id="end">End</button>
    </div>
  </div>

  <script>
    const startBtn = document.getElementById('start');
const holdBtn = document.getElementById('hold');
const endBtn = document.getElementById('end');
const output = document.getElementById('output');
const glowIndicator = document.getElementById('glow-indicator');

let recognition;
let isListening = false;
let audioContext;
let analyser;
let microphone;

// Initialize Web Speech API and Audio Context
const initRecognition = () => {
  window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

  // Initialize Speech Recognition
  recognition = new SpeechRecognition();
  recognition.lang = 'en-US';
  recognition.interimResults = false;
  recognition.continuous = true;

  // Speech Start Event
  recognition.onstart = () => {
    isListening = true;
    glowIndicator.style.opacity = "1";
    output.textContent = "Listening...";
    startAudioVisualization();
  };

  // Process Speech Result
  recognition.onresult = (event) => {
    const transcript = event.results[event.results.length - 1][0].transcript.trim();
    output.textContent = `You said: "${transcript}"`;

    if (transcript.toLowerCase() === 'hold') {
      recognition.stop();
      output.textContent = "Paused. Say 'Start' to continue.";
      stopAudioVisualization();
      return;
    }

    if (transcript.toLowerCase() === 'end') {
      recognition.stop();
      output.textContent = "Session Ended.";
      stopAudioVisualization();
      return;
    }

    output.textContent = `Processing: "${transcript}"`;
    fetchOpenAIResponse(transcript);
  };

  recognition.onerror = () => {
    output.textContent = "Error occurred. Please try again.";
    stopAudioVisualization();
  };

  recognition.onend = () => {
    isListening = false;
    glowIndicator.style.opacity = "0";
    stopAudioVisualization();
  };
};

// Start Visualizing Audio Input
const startAudioVisualization = async () => {
  audioContext = new (window.AudioContext || window.webkitAudioContext)();
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  microphone = audioContext.createMediaStreamSource(stream);
  analyser = audioContext.createAnalyser();
  microphone.connect(analyser);
  analyser.fftSize = 256;

  const dataArray = new Uint8Array(analyser.frequencyBinCount);
  const visualize = () => {
    analyser.getByteFrequencyData(dataArray);
    const volume = Math.max(...dataArray) / 255;
    glowIndicator.style.boxShadow = `0 0 ${30 + volume * 50}px ${10 + volume * 20}px rgba(0, 123, 255, ${0.5 + volume * 0.5})`;
    if (isListening) requestAnimationFrame(visualize);
  };
  visualize();
};

const stopAudioVisualization = () => {
  if (audioContext) {
    audioContext.close();
    glowIndicator.style.boxShadow = "0 0 30px 10px rgba(0, 123, 255, 0.5)";
  }
};

// OpenAI Integration
const fetchOpenAIResponse = async (query) => {
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer YOUR_OPENAI_API_KEY`
    },
    body: JSON.stringify({
      model: 'gpt-4',
      messages: [{ role: 'user', content: query }]
    })
  });
  const data = await response.json();
  output.textContent = data.choices[0]?.message?.content || "No response from OpenAI.";
};

// Control Buttons
startBtn.addEventListener('click', () => {
  if (!isListening) {
    initRecognition();
    recognition.start();
  }
});

holdBtn.addEventListener('click', () => {
  if (isListening) recognition.stop();
});

endBtn.addEventListener('click', () => {
  if (isListening) recognition.stop();
});

  </script>
</body>
</html>
