<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Vox - AI Assistant</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    @keyframes glow {
      0% { box-shadow: 0 0 5px #4f46e5; }
      50% { box-shadow: 0 0 20px #4f46e5, 0 0 30px #818cf8; }
      100% { box-shadow: 0 0 5px #4f46e5; }
    }
    .glow-effect {
      animation: glow 2s infinite;
    }
    .visualizer-bar {
      transition: height 0.1s ease;
    }
  </style>
</head>
<body class="bg-gray-900 text-white min-h-screen flex flex-col items-center justify-center p-4">
  <!-- Notification Banner -->
  <div class="fixed top-0 left-0 right-0 bg-gradient-to-r from-purple-500 via-pink-500 to-red-500 text-white py-2 px-4 text-center font-bold shadow-lg z-50">
    Welcome to Vox, your AI-powered voice assistant!
  </div>

  <div class="w-full max-w-2xl mx-auto mt-16">
    <!-- Status Indicator -->
    <div id="status-indicator" class="text-center font-bold text-xl mb-4">Idle</div>

    <!-- Audio Visualizer -->
    <div class="relative h-32 bg-gray-800 rounded-lg overflow-hidden mb-6">
      <div id="audio-visualizer" class="absolute inset-0 flex items-end justify-around">
        <!-- Visualizer bars will be dynamically added here -->
      </div>
    </div>

    <!-- Chat Section -->
    <div class="bg-gray-800 rounded-lg p-4 mb-6 h-64 overflow-y-auto" id="chat-container">
      <!-- Chat messages will appear here -->
    </div>

    <!-- Control Buttons -->
    <div class="flex justify-center space-x-4">
      <button id="start-btn" class="bg-green-500 hover:bg-green-600 text-white font-bold py-2 px-4 rounded-full transition duration-300 ease-in-out transform hover:scale-105 focus:outline-none focus:ring-2 focus:ring-green-500 focus:ring-opacity-50">
        Start Listening
      </button>
      <button id="hold-btn" class="bg-yellow-500 hover:bg-yellow-600 text-white font-bold py-2 px-4 rounded-full transition duration-300 ease-in-out transform hover:scale-105 focus:outline-none focus:ring-2 focus:ring-yellow-500 focus:ring-opacity-50">
        Hold
      </button>
      <button id="end-btn" class="bg-red-500 hover:bg-red-600 text-white font-bold py-2 px-4 rounded-full transition duration-300 ease-in-out transform hover:scale-105 focus:outline-none focus:ring-2 focus:ring-red-500 focus:ring-opacity-50">
        End Session
      </button>
    </div>
  </div>

  <script>
    // OpenAI API key (securely stored)
    const OPENAI_API_KEY = "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";

    // Webkit Speech Recognition setup
    let recognition;
    try {
      recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.continuous = true;
      recognition.interimResults = false;
      recognition.lang = "en-US";
    } catch (e) {
      console.error("Speech recognition not supported:", e);
      alert("Speech recognition is not supported in this browser. Please try using Google Chrome.");
    }

    let isListening = false;
    let isHolding = false;

    const startBtn = document.getElementById("start-btn");
    const holdBtn = document.getElementById("hold-btn");
    const endBtn = document.getElementById("end-btn");
    const statusIndicator = document.getElementById("status-indicator");
    const chatContainer = document.getElementById("chat-container");
    const audioVisualizer = document.getElementById("audio-visualizer");

    // Start Listening
    function startListening() {
      if (!recognition) return;
      if (!isListening && !isHolding) {
        try {
          recognition.start();
          isListening = true;
          updateStatus("Listening...");
          startBtn.classList.add("glow-effect");
          startAudioVisualization();
        } catch (e) {
          console.error("Error starting recognition:", e);
          updateStatus("Error: Could not start listening");
        }
      }
    }

    // Hold Listening
    function holdListening() {
      if (!recognition) return;
      if (isListening) {
        recognition.stop();
        isListening = false;
        isHolding = true;
        updateStatus("On Hold");
        startBtn.classList.remove("glow-effect");
        holdBtn.classList.add("glow-effect");
        stopAudioVisualization();
      } else if (isHolding) {
        try {
          recognition.start();
          isListening = true;
          isHolding = false;
          updateStatus("Listening...");
          startBtn.classList.add("glow-effect");
          holdBtn.classList.remove("glow-effect");
          startAudioVisualization();
        } catch (e) {
          console.error("Error resuming recognition:", e);
          updateStatus("Error: Could not resume listening");
        }
      }
    }

    // Stop Listening
    function stopListening() {
      if (!recognition) return;
      if (isListening || isHolding) {
        recognition.stop();
        isListening = false;
        isHolding = false;
        updateStatus("Idle");
        startBtn.classList.remove("glow-effect");
        holdBtn.classList.remove("glow-effect");
        stopAudioVisualization();
      }
    }

    // Display message in the chat
    function displayMessage(sender, message) {
      const messageElement = document.createElement("div");
      messageElement.className = `mb-2 p-2 rounded-lg ${sender === 'You' ? 'bg-blue-600 text-white self-end' : 'bg-gray-700 text-white self-start'}`;
      messageElement.textContent = `${sender}: ${message}`;
      chatContainer.appendChild(messageElement);
      chatContainer.scrollTop = chatContainer.scrollHeight;
    }

    // Use OpenAI API to generate a response
    async function fetchOpenAIResponse(userMessage) {
      try {
        const response = await fetch("https://api.openai.com/v1/chat/completions", {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${OPENAI_API_KEY}`,
          },
          body: JSON.stringify({
            model: "gpt-3.5-turbo",
            messages: [{ role: "user", content: userMessage }],
          }),
        });

        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`);
        }

        const data = await response.json();
        return data.choices[0].message.content.trim();
      } catch (error) {
        console.error('Error fetching OpenAI response:', error);
        return "I'm sorry, I encountered an error while processing your request. Please try again later.";
      }
    }

    // Text-to-Speech
    function speak(text) {
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = "en-US";
      window.speechSynthesis.speak(utterance);
    }

    // Handle voice recognition results
    if (recognition) {
      recognition.onresult = async (event) => {
        const transcript = event.results[event.results.length - 1][0].transcript.trim();
        displayMessage("You", transcript);

        const botResponse = await fetchOpenAIResponse(transcript);
        displayMessage("Vox", botResponse);
        speak(botResponse);
      };

      recognition.onerror = (event) => {
        console.error("Recognition error:", event.error);
        updateStatus("Error: " + event.error);
      };
    }

    // Update status in UI
    function updateStatus(status) {
      statusIndicator.textContent = status;
    }

    // Audio Visualization
    let audioContext, analyser, dataArray, source;
    const NUM_BARS = 64;

    function initAudioVisualization() {
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      dataArray = new Uint8Array(analyser.frequencyBinCount);

      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          source = audioContext.createMediaStreamSource(stream);
          source.connect(analyser);
        })
        .catch(err => {
          console.error('Error accessing microphone:', err);
          updateStatus("Error: Microphone access denied");
        });

      // Create visualizer bars
      for (let i = 0; i < NUM_BARS; i++) {
        const bar = document.createElement('div');
        bar.className = 'visualizer-bar bg-purple-500 w-1';
        audioVisualizer.appendChild(bar);
      }
    }

    function startAudioVisualization() {
      if (!audioContext) initAudioVisualization();
      updateVisualization();
    }

    function stopAudioVisualization() {
      if (source) {
        source.disconnect();
        source = null;
      }
      const bars = audioVisualizer.children;
      for (let bar of bars) {
        bar.style.height = '0%';
      }
    }

    function updateVisualization() {
      if (!isListening) return;

      analyser.getByteFrequencyData(dataArray);
      const bars = audioVisualizer.children;
      
      for (let i = 0; i < NUM_BARS; i++) {
        const barHeight = (dataArray[i] / 255) * 100;
        bars[i].style.height = barHeight + '%';
      }

      requestAnimationFrame(updateVisualization);
    }

    // Event listeners
    startBtn.addEventListener("click", startListening);
    holdBtn.addEventListener("click", holdListening);
    endBtn.addEventListener("click", stopListening);

    // Initialize
    initAudioVisualization();
  </script>
</body>
</html>
